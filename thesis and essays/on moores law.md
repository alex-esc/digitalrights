# On Moore's law

A factoid known to many tech enthusiasts is Moore's law. Emphasis is required on factoid, since Moore's Law can no longer be seen as a pure fact, not in the empirically true sense nor in the validity of the created expectations my itself; Moore's law is more of a piece of history, like the ancient Greek understanding of the atom (that which can not be further divided) versus Bohr's atomic model or todays discoveries in quantum physics, Moore's law has therefore downgraded from fact to factoid, interesting but trivial, thought provoking but useless. Now, what was Moore's law?

If our understanding of atoms as the most basic building block guides our perspective in natural philosophy, then understanding computers in terms of micro-chips as the most basic unit of modern computing appears to logically follow. However, this position is overtly simplistic, just like the world is more than just the sum of its atomic parts, computers are more than the amount of horse power stuffed in the silicon chip. In 1965 Gordon Moore, whom will later co-found Intel, published a speculative article on a radio focused magazine titled *Electronics*. In this article Moore dreams of the endless possibilities enabled by evermore complex integrated circuits. The article was tittled "*Cramming more components onto integrated circuits*" and the thesis was not so far off from the headline: That approximately every two years, thanks to technical and economical developments, we are able to cram more transistors into a single computer chip. Thus a new expectation was set for the world of personal computers: We will see double the power and speed in the next two years, next generation devices will be twice as powerful as the previous generation, and the same will happen on the next generation, and the next after that, and so on.

With this in mind, the world of technology could only grow and expand. This sentiment is partly behind all kinds of tech disasters like the dot com boom and bust of the early 2000's. Digital has this engravement for growth, social media is supposed to one day have every single human registered, internet of things devices are supposed to one day fully replace all "non-smart" objects, all jobs are to be fully automated one day, we're supposed to grow and evolve so fast that uploading our consciousness to silicon chips and achieving immortality will happen in our lifetimes. But like the dot com bubble, our hopes for endless balloning growth will eventually pop and come crashing down while performing comedic high pitched sounds. To briefly explain the dot com boom and bust, with the turn of the century a new horizon lied ahead, this new world was to be mainly lead by digital technologies, mainly internet based businesses. Previously a business needed endless planing and fiddling of strategy to find a successful niche in the market, a deep understanding of economics and finance; but the promise of digital technology and the internet changed that notion. Now anyone with an idea and a computer could create a product and market himself with a website - no more complex organizing of the brand, marketing and so on. Now thanks to the Web, the best ideas could advertise themselves, find their own market since *dog* buyers would naturally visit `dogs.com`, buyers would e-mail each-other creating clout and hype for products and so on. The promise was that since now any 16 year old with an internet connection could focus solely on his product or idea and leave economics and marketing to users to self sort themselves, the new entrepreneurs would have platform without barriers to innovate, create new products and subvert the market. But the harsh truth is that if any 16 year old has a platform for his idea, the market will inevitably get flooded with sub par ideas. But the internet was a place for the nerds and the computer literate, from the outside it all looked like magic, a spectacle was formed. Investors whom didn't understand digital technology looked at internet based companies as inherently revolutionary. Bread makers can now sell their product to anyone in the World (Wide Web), this in the eyes of crony capitalists meant the entire population would become customers - a white lie told by tech opportunists and digital tricksters. The combination between ignorant investors (literate in regards of the technology behind the scenes or economically literate) begun to trow comical amount of money at website based companies, thus popularly called the dot com boom. dot com companies thrived, not by merit, but by speculation. This created an economic bubble, the hype behind dot com businesses made them seem like a good investment, then after getting a few million dollars those companies became actually valuable to invest in, therefore more people invested, with even more investors the companies became even more worthy of further investing since they were thriving. Superficially they thrived if only observing the stock market, if one looked at the products from dot com organizations little innovation took place. Eventually the bubble could no longer maintain itself, when investors noticed no real value or growth was being produced, only perceived, their investments were withdrawn. And in a rapid and violent moment of surprise, the value of most online companies plummeted, ending the cycle of hype, growth and bust of any economic bubble.

The idea that dot com growth would forever continue is, in part, feed by the notion of Moore's law. If computers would endlessly improve, then computerized markets would too. But the realists whom understand Moore's law beyond the motto of "*computer processors get faster every two years*" know there is a fatal flaw built into Moore's observations. This flaw comes back to our initial analogy: Atoms and the physical world. Moore admitted that this phenomenon had more to do with "stuffing" components into chips and less with an intrinsic doubling of horse power. Generally speaking, the more transistors in one chip the better, more components means a chip can do more operations at a time, hold bigger values and therefore solve ever more complex problems. But there is a physical limit to all of this. Modern processors have become faster and faster precisely because we have engineered increasingly small transistors. We shrinked transistors from taking centimeters down to milliliters and now, to irresponsibly simplify, to be to small to function. Take for example, and this is a simplified example, that a transistor, is of course, made of atoms, but the smaller you make it you will hit the point when you find the minimum amount of atoms to arrange and make up a transistor - once we get atomic, we cant go smaller. Add to that the issue of heat, each process on a computer is essentially an exchange of energy, and heat is often produced in every operation. Meaning a highly transistor dense chip would melt itself away. This is all built into Moore's observations, in the 1965 paper laying his theories, Moore does admit the heating problem and the economic impossibility of creating the ideal chip. But Moore's law by simply being referred to as a law, sets a narrative of exponential growth.

This is the same trap faced by many observant algebra students. When thought of basic graphs and plotting lines with slopes and parabolas it then becomes extremely tempting for the teacher to create exercises with real world examples. A typical math problem may read like this: *A bridge will be constructed, and the bridge must be as high for a bus 5 times 10 meter buss to pass through it, calculate the parabola of the bridge using the standard y=ax^2 + bx + c formula*. Ones who remember their parabolas will quickly notice that this naive formulation for an exercise is well intentioned, it provides with some basic "real world use" for the formula thought in class and with simple narrative background of why you are solving the equation. But a literal interpretation of the situation and constraints given by the problem makes the issue at hand an impossibility, since the formula provided will only output parabolas that extend into infinity. A real world bridge that is tall enough for buses to pass by it will not go in for infinity, not with the formula provided at least. In this case, or cyclical math eye warns of the impossibility of a forever-bridge. We must use this very same notion when inspecting any kind of graph that aims to predict the future. Let's use another hypothetical example of bad prediction thanks to graph interpretation: Male athletes outrun women athletes today, but recent trends suggests that someday in the next 15 years women will improve faster than men will, thus we can extrapolate that in the next 15 years women will be faster than man (this is a made up example, with made up numbers, but do follow along). The conclusion may be based in empirical facts, it's true that women are becoming faster, but we can only use this linear function to extrapolate so much, it would then be unsound to claim that since the slope of the line trends upwards that one day both women and man will run faster than light. To follow the positive curve to absurd amounts means to disassociate so much from reality than we are no longer studding real world performance of man and women in sports, but instead we're fighting a fake war of abstract graphs and numbers. Since the slope is positive, and we blindly trust the numbers it it will generate such dadaists results, if we go the other way around, we will also extrapolate that at some point in the distant past man and women were incapable of running, since their speeds were zero at some point in history.

In the same way we can come to misinterpret Moore's law, if we understand computer chips as getting exponentially more transistors per two years we can plot this into a graph. And thanks to the absurdity of infinity growing graphs and functions we can then conclude that computers will improve forever. But the more we go of into infinities, be it negative or positive, we will find the same surreal conclusions of mapping the real world into a logarithmic graph. If we look far enough into the positive *X* axis we will find that according to this graph, computer chips will have more transistors than there are atoms in the universe. Foolishly enough if we look into the negative *X* axis we can infer that there was a time (negative time) when transistors inside computer chips were bigger than the observable universe. This is the silly reality about mapping physical and material events into a graph and then trying to predict in absolutes. Of course nobody claimed that Moore's law predicted that we will someday find an ancient chip with transistors the size of a bus, or that we will someday build a chip bigger than planet earth, but a lesser extreme is indirectly claimed by the absurd notion of infinity. We literally thought the Web will create infinitively growing market, the dot com hype was real and based partly in the notion of absolute infinite growth. To then believe the nonsense of technologists and their talking heads that the singularity is out to get us and that we can automate your life away is equally absurd. Elon Musk wants to build a Martian society to save us from our rotten, polluted and gloaming earth, but he cant even make a hole in the ground with his appropriately named *boring company*. If we bet our all on Elon or other monolithic figure or app to save us we will hit the same wall from the dot com era, the same wall we're experiencing with the physical constraints of Moore's law and the same wall we hit everyday when our technologies actively predict our behavior. This is the natural consequence of digitizing everything, the manufactured expectation for infinite growth is baked into our devices, how they operate, how they are expected to perform on the market and so on. These devices are made in an environment that presupposes infinite growth, thus they can only play out this logic and engrave it on it's users. Our economy is based on speculation, thus the products they create work on speculation and data analysis. The arrival of big data into the playing field is anything but a surprise given the ideology underneath.